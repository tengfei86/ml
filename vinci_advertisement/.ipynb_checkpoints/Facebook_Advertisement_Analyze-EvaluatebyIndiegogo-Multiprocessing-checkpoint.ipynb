{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'indiegogo',\n",
       " 'export_20181106_1552.csv',\n",
       " 'Ad-Sets-Oct-19-2018-Oct-29-2018.csv',\n",
       " 'indiegogo_orders.csv',\n",
       " 'KAG_conversion_data.csv',\n",
       " 'data_rule.csv',\n",
       " 'Vinci-Report-Oct-19-2018-Oct-24-2018.csv',\n",
       " 'Inspero-Inc.-0927-86396-03-Ad-Sets-Lifetime.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_row',100)\n",
    "pd.set_option('display.max_columns',400)\n",
    "pd.set_option('display.float_format','{:20,.2f}'.format)\n",
    "pd.set_option('display.max_colwidth',600)\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn import ensemble\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pympler import tracker\n",
    "os.listdir(\"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawbar(col):\n",
    "    data = df[col].value_counts().reset_index().rename(columns = {col:'count'})\n",
    "    sns.barplot(x = 'count',y = 'index',data = data[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_horizontal_bar_compare_roas(col,df):\n",
    "    d = df[['ROAS',col]].groupby(col).agg({'ROAS': 'mean'}).reset_index().sort_values(by = 'ROAS',ascending = False)[:20]\n",
    "    sns.barplot(x = 'ROAS',y = col,data = d,orient = 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_rules_cols = ['Campaign Name','Ad Set ID','Ad Set Name','Age Max','Age Min','Countries','Custom Audiences','Gender','Flexible Inclusions','Product 1 - Link','Publisher Platforms','Facebook Positions','Instagram Positions','Device Platforms','Title','Body']\n",
    "\n",
    "# new1  \n",
    "# rules\n",
    "df_indiegogo_rules_new1 = pd.read_csv('./data/indiegogo/new1_setting_utf8.csv',sep = '\\t',dtype = {'Ad Set ID':'str','Custom Audiences':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_new1 = df_indiegogo_rules_new1[df_indiegogo_rules_new1[\"Campaign Name\"].str.contains('indie',case = False)]\n",
    "df_indiegogo_rules_new1 = df_indiegogo_rules_new1[used_rules_cols]\n",
    "# data\n",
    "df_indiegogo_data_new1 = pd.read_csv('./data/indiegogo/new1_data_utf8.csv',sep = ',',dtype = {'Ad Set ID':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_new1['Ad Set ID'] = df_indiegogo_rules_new1['Ad Set ID'].apply(lambda x: x.split(':')[1])\n",
    "# summary\n",
    "data_new1 = df_indiegogo_rules_new1.merge(df_indiegogo_data_new1,how = 'left',on = 'Ad Set ID')\n",
    "#==============================================================================================================\n",
    "\n",
    "# new2 \n",
    "# rules\n",
    "df_indiegogo_rules_new2 = pd.read_csv('./data/indiegogo/new2_setting_utf8.csv',sep = '\\t',dtype = {'Ad Set ID':'str','Custom Audiences':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_new2 = df_indiegogo_rules_new2[df_indiegogo_rules_new2[\"Campaign Name\"].str.contains('indie',case = False)]\n",
    "df_indiegogo_rules_new2 = df_indiegogo_rules_new2[used_rules_cols]\n",
    "# data\n",
    "df_indiegogo_data_new2 = pd.read_csv('./data/indiegogo/new2_data_utf8.csv',sep = ',',dtype = {'Ad Set ID':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_new2['Ad Set ID'] = df_indiegogo_rules_new2['Ad Set ID'].apply(lambda x: x.split(':')[1])\n",
    "# summary\n",
    "data_new2 = df_indiegogo_rules_new2.merge(df_indiegogo_data_new2,how = 'left',on = 'Ad Set ID')\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "\n",
    "# old2 \n",
    "# rules\n",
    "df_indiegogo_rules_old2 = pd.read_csv('./data/indiegogo/old2_setting_utf8.csv',sep = '\\t',dtype = {'Ad Set ID':'str','Custom Audiences':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_old2 = df_indiegogo_rules_old2[df_indiegogo_rules_old2[\"Campaign Name\"].str.contains('indie',case = False)]\n",
    "df_indiegogo_rules_old2 = df_indiegogo_rules_old2[used_rules_cols]\n",
    "# data\n",
    "df_indiegogo_data_old2 = pd.read_csv('./data/indiegogo/old2_data_utf8.csv',sep = ',',dtype = {'Ad Set ID':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_old2['Ad Set ID'] = df_indiegogo_rules_old2['Ad Set ID'].apply(lambda x: x.split(':')[1])\n",
    "# summary\n",
    "data_old2 = df_indiegogo_rules_old2.merge(df_indiegogo_data_old2,how = 'left',on = 'Ad Set ID')\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "# old3 \n",
    "# rules\n",
    "df_indiegogo_rules_old3 = pd.read_csv('./data/indiegogo/old3_setting_utf8.csv',sep = '\\t',dtype = {'Ad Set ID':'str','Custom Audiences':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_old3 = df_indiegogo_rules_old3[df_indiegogo_rules_old3[\"Campaign Name\"].str.contains('indie',case = False)]\n",
    "df_indiegogo_rules_old3 = df_indiegogo_rules_old3[used_rules_cols]\n",
    "# data\n",
    "df_indiegogo_data_old3 = pd.read_csv('./data/indiegogo/old3_data_utf8.csv',sep = ',',dtype = {'Ad Set ID':'str'},encoding = 'utf-8')\n",
    "df_indiegogo_rules_old3['Ad Set ID'] = df_indiegogo_rules_old3['Ad Set ID'].apply(lambda x: x.split(':')[1])\n",
    "# summary\n",
    "data_old3 = df_indiegogo_rules_old3.merge(df_indiegogo_data_old3,how = 'left',on = 'Ad Set ID')\n",
    "#==============================================================================================================\n",
    "\n",
    "\n",
    "data = pd.concat([data_new1,data_new2,data_old2,data_old3],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_interest(x):\n",
    "    if pd.notna(x):\n",
    "        keys = x.keys()\n",
    "        for key_num,key in enumerate(keys):\n",
    "            if key_num != 0:\n",
    "                result = \"|\" + key + \":\" + x[key][0]['name']\n",
    "            else:\n",
    "                result = key + \":\" + x[key][0]['name']\n",
    "            for i in range(1,len(x[key])):\n",
    "                result = result + '_' + x[key][i]['name']\n",
    "    else:\n",
    "        return ''\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and feature engineering\n",
    "# step 1 change column name\n",
    "data.rename(columns = {'Flexible Inclusions':'Interest','CPM (Cost per 1,000 Impressions) (USD)':'CPM','CTR (Link Click-Through Rate)':'CTR','CPC (Cost per Link Click) (USD)':'CPC','Amount Spent (USD)':'Spent','Website Purchases':'Conversions','Cost per Purchase (USD)':'CPR','Website Purchase ROAS (Return on Ad Spend)':'ROAS','Product 1 - Link':'Product Link','Ad Set Name_x':'Ad Set Name','Reporting Starts':'Day'},inplace = True)\n",
    "    \n",
    "# step 2 drop unused columns\n",
    "unused_cols = ['Ad Set Name_y','Reporting Ends','Budget','Budget Type']\n",
    "data.drop(columns = unused_cols,inplace = True)   \n",
    "# step 3 null values\n",
    "cols_nan_zeros = ['CTR','Link Clicks','Conversions','Website Purchases Conversion Value','Landing Page Views','ROAS']\n",
    "for col in cols_nan_zeros:\n",
    "    data[col].fillna(0,inplace = True)    \n",
    "data['Gender'].fillna('All',inplace = True)    \n",
    "# step 4  interest json expand\n",
    "data['Interest'] = data['Interest'].apply(lambda x: json.loads(x)[0] if pd.notna(x) else None)\n",
    "# data['Interest'] = data['Interest'].apply(lambda z: z['interests'][0]['name'] + '_' + z['interests'][1]['name'] if not pd.isnull(z) else '') \n",
    "data['Interest'] = data['Interest'].apply(apply_interest) \n",
    "# step 5 date format\n",
    "data['Day'] = pd.to_datetime(data['Day'])\n",
    "data['DayOfWeek'] = data['Day'].dt.dayofweek\n",
    "data['DayOfMonth'] = data['Day'].dt.day\n",
    "data['Month'] = data['Day'].dt.month\n",
    "# step 6 contries format\n",
    "data['Countries'] = data['Countries'].astype(str).apply(lambda c: c.replace(',','_').replace(' ',''))\n",
    "# step 7 age format to str and daily budget\n",
    "data['Age Max'] = data['Age Max'].astype('str')\n",
    "data['Age Min'] = data['Age Min'].astype('str')\n",
    "# step 8  apply lookalike rules to new features\n",
    "pattern = re.compile(r\"^(\\w+) \\([\\w ]+ (\\d{1,2}%)\\) \\- (\\w+)$\")\n",
    "# pattern_pixel = re.compile(r\"^ERL\\_[purchase|checkout|payment](\\w+)$\")\n",
    "data['Custom Audiences'] = data['Custom Audiences'].apply(lambda x: x.split(':')[1] if pd.notna(x) else None)\n",
    "data['Group.Range'] = data['Custom Audiences'].apply(lambda x : pattern.match(x).group(2)   if pd.notna(x) and pattern.match(x) is not None else '')\n",
    "data['Group.Source'] = data['Custom Audiences'].apply(lambda x : pattern.match(x).group(3) if pd.notna(x) and pattern.match(x) is not None else '')\n",
    "data['IsLookalike'] = data['Custom Audiences'].apply(lambda x : 1 if pd.notna(x) else 0)\n",
    "# step 9 CR\n",
    "data['CR'] = data['Conversions'] / data['Link Clicks']\n",
    "# step 10 filled na because of no people\n",
    "cols_nan_nopeople = ['CPC','CPR','CR']\n",
    "for col in cols_nan_nopeople:\n",
    "    data[col] = data[col].fillna(-1) \n",
    "# step 11\n",
    "platform_cols = ['Publisher Platforms','Facebook Positions','Instagram Positions','Device Platforms']\n",
    "for col in platform_cols:\n",
    "    data[col] = data[col].fillna(\"\")\n",
    "# step 12 day NAT deleted\n",
    "data = data[data['Day'].notnull()]\n",
    "\n",
    "# step 13  title body \n",
    "# data['Title'].fillna(\"\",inplace = True)\n",
    "# data['Body'].fillna(\"\",inplace = True)\n",
    "# vectorizer_title = TfidfVectorizer(max_features = 12)\n",
    "# vectorizer_title.fit(data['Title'])\n",
    "# df_title_tfidf = pd.DataFrame(vectorizer_title.transform(data['Title']).toarray(),dtype = 'float16',index = data.index)\n",
    "# df_title_tfidf.columns = [ 'tfidf_title_' + str(id + 1) for id in range(12)]\n",
    "\n",
    "# vectorizer_body = TfidfVectorizer(max_features = 12)\n",
    "# vectorizer_body.fit(data['Body'])\n",
    "# df_body_tfidf = pd.DataFrame(vectorizer_body.transform(data['Body']).toarray(),dtype = 'float16',index = data.index)\n",
    "# df_body_tfidf.columns = [ 'tfidf_body_' + str(id + 1) for id in range(12)]\n",
    "\n",
    "# data = pd.concat([data,df_title_tfidf,df_body_tfidf],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_cols = ['Day','Ad Set ID','Custom Audiences','ROAS','Campaign Name','Title','Body','Ad Set Name']\n",
    "real_cols = [col for col in data.columns if data[col].dtype != 'object' and col not in excluded_cols]\n",
    "# real_cols_nottfidf = [col for col in real_cols if 'tfidf_' not in col]\n",
    "# real_cols_tfidf = [col for col in real_cols if 'tfidf_' in col]\n",
    "category_cols = ['Age Max','Age Min','Countries','Gender','Interest','Product Link','DayOfWeek','DayOfMonth','Month','Group.Range','Group.Source','IsLookalike']    \n",
    "data[real_cols].hist(bins=10,figsize = (15,35),layout=(12,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display columns relation\n",
    "draw_horizontal_bar_compare_roas('Interest',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Countries',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come to count  \n",
    "grouped_bydayofweek = data.loc[data.ROAS > 0,['Countries','ROAS']].groupby('Countries').count().reset_index().sort_values(['ROAS'],ascending = False)[:20]\n",
    "sns.barplot(x = 'ROAS',y = 'Countries',data = grouped_bydayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Age Max',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come to count  \n",
    "grouped_byagemax = data.loc[data.ROAS > 0,['Age Max','ROAS']].groupby('Age Max').count().reset_index().sort_values(['ROAS'],ascending = False)[:20]\n",
    "sns.barplot(x = 'ROAS',y = 'Age Max',data = grouped_byagemax,orient = 'h')\n",
    "grouped_byagemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Group.Range',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Group.Source',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Publisher Platforms',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Facebook Positions',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Instagram Positions',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_horizontal_bar_compare_roas('Device Platforms',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(x = 'DayOfWeek',y = 'ROAS',hue = 'Countries',data = data)\n",
    "grouped_bydayofweek = data.loc[data.ROAS > 0,['DayOfWeek','ROAS']].groupby('DayOfWeek').count().reset_index()\n",
    "sns.barplot(x = 'DayOfWeek',y = 'ROAS',data = grouped_bydayofweek )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupbyday_cr = data[['CR','Day']].groupby('Day').mean().reset_index()\n",
    "# sns.lineplot(x = 'Day',y = 'CR',data= groupbyday_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupbyday_cr = data[['CR','Product Link']].groupby('Product Link').mean().reset_index()\n",
    "# sns.lineplot(x = 'Product Link',y = 'CR',data= groupbyday_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data[category_cols]\n",
    "for col in category_cols:\n",
    "    data_X[col] = preprocessing.LabelEncoder().fit_transform(data_X[col].astype(str))\n",
    "data_y = data['ROAS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,test_X,train_y,test_y = train_test_split(data_X,data_y,test_size = 0.2,random_state = 1986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10],'epsilon':[0.1,0.3]}\n",
    "# svr = SVR(gamma='scale')\n",
    "\n",
    "# clf = GridSearchCV(svr,parameters,cv = 5,n_jobs = -1)\n",
    "\n",
    "# clf.fit(train_X,train_y)\n",
    "\n",
    "# clf.best_params_\n",
    "\n",
    "# clf.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr = ensemble.RandomForestRegressor(max_depth=10, random_state=0,n_estimators=500)\n",
    "# regr.fit(train_X, train_y)\n",
    "# mse = mean_squared_error(test_y, regr.predict(test_X))\n",
    "# print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "# ols = LinearRegression()\n",
    "# ols.fit(train_X, train_y)\n",
    "# mse = mean_squared_error(test_y, ols.predict(test_X))\n",
    "# print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "\n",
    "# params = {'n_estimators': 500,'learning_rate': 0.01, 'loss': 'linear'}\n",
    "# ada = ensemble.AdaBoostRegressor(**params)\n",
    "# ada.fit(train_X, train_y)\n",
    "# mse = mean_squared_error(test_y, ada.predict(test_X))\n",
    "# print(\"MSE: %.4f\" % mse)\n",
    "# print(ada.predict(test_X))\n",
    "# print(np.array(test_y))\n",
    "\n",
    "\n",
    "# # Fit regression model\n",
    "# params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "#           'learning_rate': 0.01, 'loss': 'ls'}\n",
    "# clf = ensemble.GradientBoostingRegressor(**params)\n",
    "# clf.fit(train_X, train_y)\n",
    "# mse = mean_squared_error(test_y, clf.predict(test_X))\n",
    "# print(\"MSE: %.4f\" % mse)\n",
    "# print(clf.predict(test_X))\n",
    "# print(np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Bayesian Ridge Regression and an OLS for comparison\n",
    "bay = BayesianRidge(compute_score=True)\n",
    "bay.fit(train_X, train_y)\n",
    "mse = mean_squared_error(test_y, bay.predict(test_X))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.info()\n",
    "svr = SVR(gamma='auto', C=1.0, epsilon=0.05,kernel ='rbf') \n",
    "svr.fit(train_X,train_y)\n",
    "pred_test = svr.predict(test_X)\n",
    "pred_test[pred_test < 0] = 0\n",
    "np.log(mean_squared_error(test_y,pred_test))\n",
    "mse = mean_squared_error(test_y,pred_test)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(pred_test)\n",
    "print(np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find you\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "# list(le.classes_)\n",
    "# le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
    "# le.inverse_transform([0])\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool,Manager\n",
    "_cols = ['Age Max','Age Min','Countries','Gender','Interest','Product Link','Group.Range','Group.Source','IsLookalike']    \n",
    "# tr = tracker.SummaryTracker()\n",
    "# tr.print_diff()\n",
    "def getNextTenDays():\n",
    "    dates = []\n",
    "    today = datetime.datetime.now()\n",
    "    for n in range(1, 11):\n",
    "        dates.append(today + datetime.timedelta(days=n))\n",
    "    return dates\n",
    "\n",
    "def action(roas,keys,lock,agemax,agemin,country,gender,interest,link,islike = 0,group_range = 0,group_source = 0):\n",
    "    if islike == 1:\n",
    "        _roas = get_mean_roas(agemax,agemin,country,gender,interest,link,islike,group_range,group_source)\n",
    "    else:\n",
    "        _roas = get_mean_roas(agemax,agemin,country,gender,interest,link)\n",
    "    with lock:\n",
    "        if _roas > roas.value:\n",
    "            print(\"roas  = \",_roas)\n",
    "            roas.value = _roas\n",
    "            if islike == 1:\n",
    "                set_keys(keys,agemax,agemin,country,gender,interest,link,islike,group_range,group_source)\n",
    "            else:\n",
    "                set_keys(keys,agemax,agemin,country,gender,interest,link)          \n",
    "    print(\"1\")\n",
    "    return _roas\n",
    "    \n",
    "def get_mean_roas(agemax,agemin,country,gender,interest,link,islike = 0,group_range = 0,group_source = 0):\n",
    "    next_dates = getNextTenDays()\n",
    "    result = 0\n",
    "    date = []\n",
    "    pre_data = []\n",
    "    start = time.time()\n",
    "    for date in next_dates:\n",
    "        pre_data.append([agemax,agemin,country,gender,interest,link,date.weekday(),date.day,date.month,islike,group_range,group_source])     \n",
    "    result  =  (np.mean(bay.predict(pre_data)) + np.mean(svr.predict(pre_data))) / 2\n",
    "    end = time.time()\n",
    "    del pre_data\n",
    "    del next_dates\n",
    "    del agemax\n",
    "    del agemin\n",
    "    del country\n",
    "    del gender\n",
    "    del interest\n",
    "    del link\n",
    "    del islike\n",
    "    del group_range\n",
    "    del group_source\n",
    "    return result\n",
    "        \n",
    "def set_keys(keys,agemax,agemin,country,gender,interest,link,islike = 0,group_range = 0,group_source = 0):    \n",
    "    keys['Age Max'] = agemax\n",
    "    keys['Age Min'] = agemin\n",
    "    keys['Countries'] = country\n",
    "    keys['Gender'] = gender\n",
    "    keys['Interest'] = interest\n",
    "    keys['Product Link'] = link\n",
    "    keys['Group.Range'] = group_range\n",
    "    keys['Group.Source'] = group_source\n",
    "    keys['IsLookalike'] = islike\n",
    "def call_back(roas):\n",
    "    gc.collect()\n",
    "#     tr.print_diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output \n",
    "# union ,roas = findyou()\n",
    "# for col in _cols:\n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(data[col].astype(str))\n",
    "#     union[col] = le.inverse_transform([union[col]])[0]\n",
    "# union\n",
    "# roas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = 1\n",
    "for col in _cols:\n",
    "    print('col',col,data[col].nunique())\n",
    "    re = re * data[col].nunique()\n",
    "print(re)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager = Manager()\n",
    "# keys = manager.dict()\n",
    "# roas = manager.Value('d',0.0)\n",
    "# lock = manager.Lock()\n",
    "# pool = Pool()\n",
    "# if __name__ == '__main__':\n",
    "#     for agemax in  data_X['Age Max'].unique():\n",
    "#         for agemin in  data_X['Age Min'].unique():\n",
    "#             for country in  data_X['Countries'].unique():\n",
    "#                 for gender in data_X['Gender'].unique():\n",
    "#                     for interest in data_X['Interest'].unique():\n",
    "#                         for link in data_X['Product Link'].unique():\n",
    "#                             for islike in data_X['IsLookalike'].unique():\n",
    "#                                 if islike == 1:\n",
    "#                                     for group_range in data_X['Group.Range'].unique():\n",
    "#                                         for group_source in data_X['Group.Source'].unique():\n",
    "#                                             pool.apply_async(action,args = (roas,keys,lock,agemax,agemin,country,gender,interest,link,islike,group_range,group_source),callback= call_back)    \n",
    "#                                 else:\n",
    "#                                     group_range = 0\n",
    "#                                     group_source = 0\n",
    "#                                     pool.apply_async(action,args = (roas,keys,lock,agemax,agemin,country,gender,interest,link,islike,group_range,group_source),callback=call_back)\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# def job(x):\n",
    "#     time.sleep(1)\n",
    "#     return x*x\n",
    "# def multicore():\n",
    "#     pool = mp.Pool() \n",
    "#     res = pool.map(job, range(10))\n",
    "#     print(res)\n",
    "#     res = pool.apply_async(job, (2,))\n",
    "#     # 用get获得结果\n",
    "#     print(res.get())\n",
    "#     # 迭代器，i=0时apply一次，i=1时apply一次等等\n",
    "#     multi_res = [pool.apply_async(job, (i,)) for i in range(10)]\n",
    "#     # 从迭代器中取出\n",
    "#     print([res.get() for res in multi_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# import time\n",
    "# class someClass(object):\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def f(self, x):\n",
    "#         time.sleep(0.005)\n",
    "#         print(x*x)\n",
    "#         return x*x\n",
    "#     def go(self):\n",
    "#         pool = mp.Pool(4)\n",
    "#         pool.map(self.f, range(10))\n",
    "\n",
    "# sc = someClass()\n",
    "# sc.go()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing \n",
    "\n",
    "\n",
    "def basic_func(x):\n",
    "    if x == 0:\n",
    "        return 'zero'\n",
    "    elif x%2 == 0:\n",
    "        return 'even'\n",
    "    else:\n",
    "        return 'odd'\n",
    "\n",
    "def multiprocessing_func(x):\n",
    "    y = get_mean_roas(0,0,0,0,0,0)\n",
    "    print('{} squared results in a/an {} number'.format(x, basic_func(y)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "#     starttime = time.time()\n",
    "    pool = multiprocessing.Pool()\n",
    "    \n",
    "    for i in range(1000000):\n",
    "        start = time.time()\n",
    "        print(\"duration = \",time.time() - start)\n",
    "        pool.apply_async(multiprocessing_func, args = (i,))\n",
    "    \n",
    "    \n",
    "#     age_max_uniques = data_X['Age Max'].unique()\n",
    "#     age_min_uniques = data_X['Age Min'].unique()\n",
    "#     country_uniques = data_X['Countries'].unique()\n",
    "#     gender_uniques = data_X['Gender'].unique()\n",
    "#     interest_uniques = data_X['Interest'].unique()\n",
    "#     link_uniques = data_X['Product Link'].unique()\n",
    "#     islike_uniques = data_X['IsLookalike'].unique()\n",
    "#     group_range_uniques = data_X['Group.Range'].unique()\n",
    "#     group_source_uniques = data_X['Group.Source'].unique()\n",
    "\n",
    "\n",
    "#     for agemax in  age_max_uniques:\n",
    "#         for agemin in  age_min_uniques:\n",
    "#             for country in  country_uniques:\n",
    "#                 for gender in gender_uniques:\n",
    "#                     for interest in interest_uniques:\n",
    "#                         for link in link_uniques:\n",
    "#                             for islike in islike_uniques:\n",
    "#                                 if islike == 1:\n",
    "#                                     for group_range in group_range_uniques:\n",
    "#                                         for group_source in group_source_uniques:\n",
    "#                                             # gc.collect()\n",
    "#                                             pool.apply_async(multiprocessing_func,args = (0,),callback= call_back)\n",
    "                                            \n",
    "#                                 else:\n",
    "#                                     group_range = 0\n",
    "#                                     group_source = 0\n",
    "#                                     # gc.collect()\n",
    "#                                     pool.apply_async(multiprocessing_func,args = (0,),callback= call_back)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    \n",
    "    print('That took {} seconds'.format(time.time() - starttime))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,1],[1,0,0,0,0,0,0,0,0]]\n",
    "np.save(\"combinations.npy\", x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"b.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
