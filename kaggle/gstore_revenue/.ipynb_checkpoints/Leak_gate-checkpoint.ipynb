{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Shape: (903653, 55)\n",
      "Loaded test.csv. Shape: (804684, 53)\n",
      "train_new size 903654\n",
      "test_new size 804686\n"
     ]
    }
   ],
   "source": [
    "# As discussed in the discussion section, we have leak in the competition if we use external data.\n",
    "# D[](http://)iscussed here: https://www.kaggle.com/c/ga-customer-revenue-prediction/discussion/68235#401950\n",
    " \n",
    "#  I have downloaded the data and used it to climb to the 2nd position. Before using it I was at the 600th position. \n",
    "#  Most of the code used it from olivier's (https://www.kaggle.com/ogrellier) notebooks in this competition and I haven't done any feature engineering with the external data yet.\n",
    " \n",
    "#  You can use the data that I downloaded as it would be in the data of this kernel. \n",
    "#  There are 4 files 2 for train and 2 for test.\n",
    "#Can't upload the notebook as kaggle kernels keep on crashing but this script will get you same score as mine.\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000, \"display.max_columns\", 1000)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#using https://www.kaggle.com/julian3833/1-quick-start-read-csv-and-flatten-json-fields/notebook\n",
    "def load_df(csv_path='./all/train.csv', nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, # Important!!\n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df\n",
    "    \n",
    "train = load_df()\n",
    "test = load_df(\"./all/test.csv\")\n",
    "\n",
    "#Loading external data\n",
    "train_store_1 = pd.read_csv('./all/Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "train_store_2 = pd.read_csv('./all/Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_1 = pd.read_csv('./all/Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "test_store_2 = pd.read_csv('./all/Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "\n",
    "#Getting VisitId to Join with our train, test data\n",
    "def get_visitid(id):\n",
    "    bef_, af_ = str(x).split('.')\n",
    "    return int(bef_), (int(af_)*10 if len(af_)==1 else int(af_))\n",
    "\n",
    "for df in [train_store_1, train_store_2, test_store_1, test_store_2]:\n",
    "    df[\"visitId\"] = df[\"Client Id\"].apply(lambda x: x.split('.', 1)[1]).astype(str)\n",
    "\n",
    "train_exdata = pd.concat([train_store_1, train_store_2], sort=False)\n",
    "test_exdata = pd.concat([test_store_1, test_store_2], sort=False)\n",
    "\n",
    "for df in [train, test]:\n",
    "    df[\"visitId\"] = df[\"visitId\"].astype(str)\n",
    "\n",
    "# Merge with train/test data\n",
    "train_new = train.merge(train_exdata, how=\"left\", on=\"visitId\")\n",
    "test_new = test.merge(test_exdata, how=\"left\", on=\"visitId\")\n",
    "print(\"train_new size\",train_new.shape[0])\n",
    "print(\"test_new size\",test_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train_new.copy()\n",
    "test_copy = test_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start========================================================================\n"
     ]
    }
   ],
   "source": [
    "train_new = train_copy.copy()\n",
    "test_new = test_copy.copy()\n",
    "print(\"start========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.visitStartTime = pd.to_datetime(train_new.visitStartTime, unit='s')\n",
    "test_new.visitStartTime = pd.to_datetime(test_new.visitStartTime, unit='s')\n",
    "train_new[\"date\"] = train_new.visitStartTime\n",
    "test_new[\"date\"] = test_new.visitStartTime\n",
    "\n",
    "\n",
    "train_new.set_index(\"visitStartTime\", inplace=True)\n",
    "test_new.set_index(\"visitStartTime\", inplace=True)\n",
    "train_new.sort_index(inplace=True)\n",
    "test_new.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804686, 59)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 98 device.browser categories to 'other'; now there are 11 categories in train\n",
      "Set 15 device.operatingSystem categories to 'other'; now there are 8 categories in train\n",
      "Set 160 geoNetwork.country categories to 'other'; now there are 60 categories in train\n",
      "Set 656 geoNetwork.city categories to 'other'; now there are 77 categories in train\n",
      "Set 86 geoNetwork.metro categories to 'other'; now there are 24 categories in train\n",
      "Set 25689 geoNetwork.networkDomain categories to 'other'; now there are 62 categories in train\n",
      "Set 314 geoNetwork.region categories to 'other'; now there are 62 categories in train\n",
      "Set 5 geoNetwork.subContinent categories to 'other'; now there are 19 categories in train\n",
      "Set 46 trafficSource.adContent categories to 'other'; now there are 4 categories in train\n",
      "Set 23 trafficSource.campaign categories to 'other'; now there are 5 categories in train\n",
      "Set 2409 trafficSource.keyword categories to 'other'; now there are 8 categories in train\n",
      "Set 1 trafficSource.medium categories to 'other'; now there are 7 categories in train\n",
      "Set 2173 trafficSource.referralPath categories to 'other'; now there are 17 categories in train\n",
      "Set 303 trafficSource.source categories to 'other'; now there are 22 categories in train\n",
      "train_preprocess after (903654, 60)\n",
      "test_preprocess after (804686, 58)\n"
     ]
    }
   ],
   "source": [
    "# Drop Client Id\n",
    "for df in [train_new, test_new]:\n",
    "    df.drop(\"Client Id\", 1, inplace=True)\n",
    "\n",
    "#Cleaning Revenue\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Revenue\"].fillna('$', inplace=True)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].apply(lambda x: x.replace('$', '').replace(',', ''))\n",
    "    df[\"Revenue\"] = pd.to_numeric(df[\"Revenue\"], errors=\"coerce\")\n",
    "    df[\"Revenue\"].fillna(0.0, inplace=True)\n",
    "\n",
    "#Imputing NaN\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Sessions\"] = df[\"Sessions\"].fillna(0)\n",
    "    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].fillna(0)\n",
    "    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].fillna(0)\n",
    "    df[\"Revenue\"] = df[\"Revenue\"].fillna(0)\n",
    "    df[\"Transactions\"] = df[\"Transactions\"].fillna(0)\n",
    "    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].fillna(0)\n",
    "    df['trafficSource.adContent'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.slot'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.page'].fillna(0.0, inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.isVideoAd'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.adNetworkType'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.adwordsClickInfo.gclId'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.isTrueDirect'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.referralPath'].fillna('N/A', inplace=True)\n",
    "    df['trafficSource.keyword'].fillna('N/A', inplace=True)\n",
    "    df['totals.bounces'].fillna(0.0, inplace=True)\n",
    "    df['totals.bounces'] = df['totals.bounces'].astype(float)\n",
    "    df['totals.newVisits'].fillna(0.0, inplace=True)\n",
    "    df['totals.newVisits']  = df['totals.newVisits'].astype(float)\n",
    "    df['totals.pageviews'].fillna(0.0, inplace=True)\n",
    "    df['totals.pageviews'] = df['totals.pageviews'].astype(float)\n",
    "    df['totals.hits']  = df['totals.hits'].astype(float)\n",
    "    \n",
    "def clearRare(columnname, limit = 1000):\n",
    "    # you may search for rare categories in train, train&test, or just test\n",
    "    #vc = pd.concat([train[columnname], test[columnname]], sort=False).value_counts()\n",
    "    vc = test_new[columnname].value_counts()\n",
    "    \n",
    "    common = vc > limit\n",
    "    common = set(common.index[common].values)\n",
    "    print(\"Set\", sum(vc <= limit), columnname, \"categories to 'other';\", end=\" \")\n",
    "    \n",
    "    train_new.loc[train_new[columnname].map(lambda x: x not in common), columnname] = 'other'\n",
    "    test_new.loc[test_new[columnname].map(lambda x: x not in common), columnname] = 'other'\n",
    "    print(\"now there are\", train_new[columnname].nunique(), \"categories in train\")\n",
    "\n",
    "clearRare(\"device.browser\")\n",
    "clearRare(\"device.operatingSystem\")\n",
    "clearRare(\"geoNetwork.country\")\n",
    "clearRare(\"geoNetwork.city\")\n",
    "clearRare(\"geoNetwork.metro\")\n",
    "clearRare(\"geoNetwork.networkDomain\")\n",
    "clearRare(\"geoNetwork.region\")\n",
    "clearRare(\"geoNetwork.subContinent\")\n",
    "clearRare(\"trafficSource.adContent\")\n",
    "clearRare(\"trafficSource.campaign\")\n",
    "clearRare(\"trafficSource.keyword\")\n",
    "clearRare(\"trafficSource.medium\")\n",
    "clearRare(\"trafficSource.referralPath\")\n",
    "clearRare(\"trafficSource.source\")\n",
    "        \n",
    "# Clearing leaked data:\n",
    "for df in [train_new, test_new]:\n",
    "    df[\"Avg. Session Duration\"][df[\"Avg. Session Duration\"] == 0] = \"00:00:00\"\n",
    "    df[\"Avg. Session Duration\"] = df[\"Avg. Session Duration\"].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))\n",
    "    df[\"Bounce Rate\"] = df[\"Bounce Rate\"].astype(str).apply(lambda x: x.replace('%', '')).astype(float)\n",
    "    df[\"Goal Conversion Rate\"] = df[\"Goal Conversion Rate\"].astype(str).apply(lambda x: x.replace('%', '')).astype(float)    \n",
    "    \n",
    "del train\n",
    "del test\n",
    "train = train_new\n",
    "test = test_new\n",
    "\n",
    "print(\"train_preprocess after\",train.shape)\n",
    "print(\"test_preprocess after\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 903654 entries, 2016-08-01 07:00:12 to 2017-08-02 06:59:53\n",
      "Data columns (total 60 columns):\n",
      "channelGrouping                                      903654 non-null object\n",
      "date                                                 903654 non-null datetime64[ns]\n",
      "fullVisitorId                                        903654 non-null object\n",
      "sessionId                                            903654 non-null object\n",
      "socialEngagementType                                 903654 non-null object\n",
      "visitId                                              903654 non-null object\n",
      "visitNumber                                          903654 non-null int64\n",
      "device.browser                                       903654 non-null object\n",
      "device.browserSize                                   903654 non-null object\n",
      "device.browserVersion                                903654 non-null object\n",
      "device.deviceCategory                                903654 non-null object\n",
      "device.flashVersion                                  903654 non-null object\n",
      "device.isMobile                                      903654 non-null bool\n",
      "device.language                                      903654 non-null object\n",
      "device.mobileDeviceBranding                          903654 non-null object\n",
      "device.mobileDeviceInfo                              903654 non-null object\n",
      "device.mobileDeviceMarketingName                     903654 non-null object\n",
      "device.mobileDeviceModel                             903654 non-null object\n",
      "device.mobileInputSelector                           903654 non-null object\n",
      "device.operatingSystem                               903654 non-null object\n",
      "device.operatingSystemVersion                        903654 non-null object\n",
      "device.screenColors                                  903654 non-null object\n",
      "device.screenResolution                              903654 non-null object\n",
      "geoNetwork.city                                      903654 non-null object\n",
      "geoNetwork.cityId                                    903654 non-null object\n",
      "geoNetwork.continent                                 903654 non-null object\n",
      "geoNetwork.country                                   903654 non-null object\n",
      "geoNetwork.latitude                                  903654 non-null object\n",
      "geoNetwork.longitude                                 903654 non-null object\n",
      "geoNetwork.metro                                     903654 non-null object\n",
      "geoNetwork.networkDomain                             903654 non-null object\n",
      "geoNetwork.networkLocation                           903654 non-null object\n",
      "geoNetwork.region                                    903654 non-null object\n",
      "geoNetwork.subContinent                              903654 non-null object\n",
      "totals.bounces                                       903654 non-null float64\n",
      "totals.hits                                          903654 non-null float64\n",
      "totals.newVisits                                     903654 non-null float64\n",
      "totals.pageviews                                     903654 non-null float64\n",
      "totals.transactionRevenue                            11516 non-null object\n",
      "totals.visits                                        903654 non-null object\n",
      "trafficSource.adContent                              903654 non-null object\n",
      "trafficSource.adwordsClickInfo.adNetworkType         903654 non-null object\n",
      "trafficSource.adwordsClickInfo.criteriaParameters    903654 non-null object\n",
      "trafficSource.adwordsClickInfo.gclId                 903654 non-null object\n",
      "trafficSource.adwordsClickInfo.isVideoAd             903654 non-null object\n",
      "trafficSource.adwordsClickInfo.page                  903654 non-null object\n",
      "trafficSource.adwordsClickInfo.slot                  903654 non-null object\n",
      "trafficSource.campaign                               903654 non-null object\n",
      "trafficSource.campaignCode                           1 non-null object\n",
      "trafficSource.isTrueDirect                           903654 non-null object\n",
      "trafficSource.keyword                                903654 non-null object\n",
      "trafficSource.medium                                 903654 non-null object\n",
      "trafficSource.referralPath                           903654 non-null object\n",
      "trafficSource.source                                 903654 non-null object\n",
      "Sessions                                             903654 non-null float64\n",
      "Avg. Session Duration                                903654 non-null int64\n",
      "Bounce Rate                                          903654 non-null float64\n",
      "Revenue                                              903654 non-null float64\n",
      "Transactions                                         903654 non-null float64\n",
      "Goal Conversion Rate                                 903654 non-null float64\n",
      "dtypes: bool(1), datetime64[ns](1), float64(9), int64(2), object(47)\n",
      "memory usage: 454.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Avg. Session Duration', 'Bounce Rate', 'Goal Conversion Rate', 'Revenue', 'Sessions', 'Transactions', 'browser.category', 'browser.os', 'campaign.medium', 'channelGrouping', 'channelGrouping_OS', 'channelGrouping_browser', 'content.source', 'device.browser', 'device.deviceCategory', 'device.isMobile', 'device.operatingSystem', 'device_deviceCategory_channelGrouping', 'geoNetwork.city', 'geoNetwork.city_device.browser', 'geoNetwork.city_device.deviceCategory', 'geoNetwork.city_device.operatingSystem', 'geoNetwork.city_trafficSource.source', 'geoNetwork.continent', 'geoNetwork.continent_device.browser', 'geoNetwork.continent_device.deviceCategory', 'geoNetwork.continent_device.operatingSystem', 'geoNetwork.continent_trafficSource.source', 'geoNetwork.country', 'geoNetwork.country_device.browser', 'geoNetwork.country_device.deviceCategory', 'geoNetwork.country_device.operatingSystem', 'geoNetwork.country_trafficSource.source', 'geoNetwork.metro', 'geoNetwork.metro_device.browser', 'geoNetwork.metro_device.deviceCategory', 'geoNetwork.metro_device.operatingSystem', 'geoNetwork.metro_trafficSource.source', 'geoNetwork.networkDomain', 'geoNetwork.networkDomain_device.browser', 'geoNetwork.networkDomain_device.deviceCategory', 'geoNetwork.networkDomain_device.operatingSystem', 'geoNetwork.networkDomain_trafficSource.source', 'geoNetwork.region', 'geoNetwork.region_device.browser', 'geoNetwork.region_device.deviceCategory', 'geoNetwork.region_device.operatingSystem', 'geoNetwork.region_trafficSource.source', 'geoNetwork.subContinent', 'geoNetwork.subContinent_device.browser', 'geoNetwork.subContinent_device.deviceCategory', 'geoNetwork.subContinent_device.operatingSystem', 'geoNetwork.subContinent_trafficSource.source', 'medium.source', 'sess_date_dow', 'sess_date_hours', 'sess_month', 'source.country', 'totals.bounces', 'totals.hits', 'totals.newVisits', 'totals.pageviews', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.isVideoAd', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.campaign', 'trafficSource.isTrueDirect', 'trafficSource.keyword', 'trafficSource.medium', 'trafficSource.referralPath', 'trafficSource.source', 'visitNumber', 'prev_session', 'next_session', 'usermean_totals.hits', 'usermean_totals.pageviews', 'usermax_visitNumber', 'hits/pageviews', 'is_high_hits']\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.57812\n",
      "[200]\tvalid_0's rmse: 1.55714\n",
      "[300]\tvalid_0's rmse: 1.55197\n",
      "[400]\tvalid_0's rmse: 1.55011\n",
      "[500]\tvalid_0's rmse: 1.54904\n",
      "[600]\tvalid_0's rmse: 1.5472\n",
      "[700]\tvalid_0's rmse: 1.54671\n",
      "[800]\tvalid_0's rmse: 1.54623\n",
      "[900]\tvalid_0's rmse: 1.54601\n",
      "[1000]\tvalid_0's rmse: 1.54622\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[925]\tvalid_0's rmse: 1.54582\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.55456\n",
      "[200]\tvalid_0's rmse: 1.53531\n",
      "[300]\tvalid_0's rmse: 1.5303\n",
      "[400]\tvalid_0's rmse: 1.52743\n",
      "[500]\tvalid_0's rmse: 1.52564\n",
      "[600]\tvalid_0's rmse: 1.52544\n",
      "Early stopping, best iteration is:\n",
      "[539]\tvalid_0's rmse: 1.52503\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.56493\n",
      "[200]\tvalid_0's rmse: 1.54266\n",
      "[300]\tvalid_0's rmse: 1.53612\n",
      "[400]\tvalid_0's rmse: 1.53222\n",
      "[500]\tvalid_0's rmse: 1.53038\n",
      "[600]\tvalid_0's rmse: 1.52889\n",
      "[700]\tvalid_0's rmse: 1.52731\n",
      "[800]\tvalid_0's rmse: 1.52728\n",
      "[900]\tvalid_0's rmse: 1.52727\n",
      "Early stopping, best iteration is:\n",
      "[863]\tvalid_0's rmse: 1.52687\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.5537\n",
      "[200]\tvalid_0's rmse: 1.53304\n",
      "[300]\tvalid_0's rmse: 1.5276\n",
      "[400]\tvalid_0's rmse: 1.52512\n",
      "[500]\tvalid_0's rmse: 1.52268\n",
      "[600]\tvalid_0's rmse: 1.52201\n",
      "[700]\tvalid_0's rmse: 1.52169\n",
      "[800]\tvalid_0's rmse: 1.52193\n",
      "Early stopping, best iteration is:\n",
      "[720]\tvalid_0's rmse: 1.5214\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's rmse: 1.58941\n",
      "[200]\tvalid_0's rmse: 1.57081\n",
      "[300]\tvalid_0's rmse: 1.56508\n",
      "[400]\tvalid_0's rmse: 1.56231\n",
      "[500]\tvalid_0's rmse: 1.56102\n",
      "[600]\tvalid_0's rmse: 1.56022\n",
      "Early stopping, best iteration is:\n",
      "[582]\tvalid_0's rmse: 1.55995\n",
      "1.5354027397339471\n"
     ]
    }
   ],
   "source": [
    "# Features Engineering\n",
    "\n",
    "\n",
    "# category features\n",
    "for df in [train, test]:\n",
    "    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n",
    "    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n",
    "    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n",
    "    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']\n",
    "\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n",
    "    \n",
    "    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n",
    "        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n",
    "            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n",
    "    \n",
    "    df['content.source'] = df['trafficSource.adContent'].astype(str) + \"_\" + df['source.country']\n",
    "    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']\n",
    "\n",
    "# date feature\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['sess_date_dow'] = df['date'].dt.dayofweek\n",
    "    df['sess_date_hours'] = df['date'].dt.hour\n",
    "    df['sess_month'] = df['date'].dt.month\n",
    "    \n",
    "# numeric lagged  featured values\n",
    "for df in [train, test]:\n",
    "    # remember these features were equal, but not always? May be it means something...\n",
    "    df[\"id_incoherence\"] = pd.to_datetime(df.visitId, unit='s') != df.date\n",
    "    # remember visitId dublicates?\n",
    "    df[\"visitId_dublicates\"] = df.visitId.map(df.visitId.value_counts())\n",
    "    # remember session dublicates?\n",
    "    df[\"session_dublicates\"] = df.sessionId.map(df.sessionId.value_counts())\n",
    "\n",
    "\n",
    "df = pd.concat([train, test])\n",
    "df.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\n",
    "df['prev_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)).astype(np.int64) // 1e9 // 60 // 60\n",
    "df['next_session'] = (df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)).astype(np.int64) // 1e9 // 60 // 60\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "train = df[:len(train)]\n",
    "test = df[len(train):]    \n",
    "\n",
    "    \n",
    "for feature in [\"totals.hits\", \"totals.pageviews\"]:\n",
    "    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].mean()\n",
    "    train[\"usermean_\" + feature] = train.fullVisitorId.map(info)\n",
    "    test[\"usermean_\" + feature] = test.fullVisitorId.map(info)\n",
    "    \n",
    "for feature in [\"visitNumber\"]:\n",
    "    info = pd.concat([train, test], sort=False).groupby(\"fullVisitorId\")[feature].max()\n",
    "    train[\"usermax_\" + feature] = train.fullVisitorId.map(info)\n",
    "    test[\"usermax_\" + feature] = test.fullVisitorId.map(info)\n",
    "    \n",
    "    \n",
    "# Dropping Constant Columns\n",
    "\n",
    "const_cols = [col for col in train.columns if len(train[col].unique())==1]\n",
    "\n",
    "for df in [train, test]:\n",
    "    df.drop(const_cols, 1, inplace=True)\n",
    "\n",
    "train.drop('trafficSource.campaignCode', 1, inplace=True)\n",
    "\n",
    "# Modeling with Olivier's method\n",
    "#https://www.kaggle.com/ogrellier/teach-lightgbm-to-sum-predictions\n",
    "\n",
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids\n",
    "\n",
    "y_reg = train['totals.transactionRevenue'].fillna(0)\n",
    "del train['totals.transactionRevenue']\n",
    "\n",
    "if 'totals.transactionRevenue' in test.columns:\n",
    "    del test['totals.transactionRevenue']\n",
    "\n",
    "excluded_features = [\n",
    "    'date', 'date_new', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n",
    "    'visitId', 'visitStartTime'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    _f for _f in train.columns\n",
    "    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n",
    "]\n",
    "\n",
    "for f in categorical_features:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['hits/pageviews'] = (df[\"totals.pageviews\"]/(df[\"totals.hits\"])).apply(lambda x: 0 if np.isinf(x) else x)\n",
    "    df['is_high_hits'] = np.logical_or(df[\"totals.hits\"]>4,df[\"totals.pageviews\"]>4).astype(np.int32)\n",
    "    df[\"Revenue\"] = np.log1p(df[\"Revenue\"])\n",
    "\n",
    "\n",
    "folds = get_folds(df=train, n_splits=5)\n",
    "y_reg = y_reg.astype(float)\n",
    "train_features = [_f for _f in train.columns if _f not in excluded_features]\n",
    "print(train_features)\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "oof_reg_preds = np.zeros(train.shape[0])\n",
    "sub_reg_preds = np.zeros(test.shape[0])\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n",
    "    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = train_features\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_reg_preds[oof_reg_preds < 0] = 0\n",
    "    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_reg_preds += np.expm1(_preds) / len(folds)\n",
    "    \n",
    "print(mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5)\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# importances['gain_log'] = np.log1p(importances['gain'])\n",
    "# mean_gain = importances[['gain', 'feature']].groupby('feature').mean()\n",
    "# importances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "# plt.figure(figsize=(8, 12))\n",
    "# sns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))\n",
    "\n",
    "\n",
    "train['predictions'] = np.expm1(oof_reg_preds)\n",
    "test['predictions'] = sub_reg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 903654 entries, 2016-08-01 07:00:12 to 2017-08-02 06:59:53\n",
      "Data columns (total 87 columns):\n",
      "Avg. Session Duration                              903654 non-null int64\n",
      "Bounce Rate                                        903654 non-null float64\n",
      "Goal Conversion Rate                               903654 non-null float64\n",
      "Revenue                                            903654 non-null float64\n",
      "Sessions                                           903654 non-null float64\n",
      "Transactions                                       903654 non-null float64\n",
      "browser.category                                   903654 non-null int64\n",
      "browser.os                                         903654 non-null int64\n",
      "campaign.medium                                    903654 non-null int64\n",
      "channelGrouping                                    903654 non-null int64\n",
      "channelGrouping_OS                                 903654 non-null int64\n",
      "channelGrouping_browser                            903654 non-null int64\n",
      "content.source                                     903654 non-null int64\n",
      "date                                               903654 non-null datetime64[ns]\n",
      "device.browser                                     903654 non-null int64\n",
      "device.deviceCategory                              903654 non-null int64\n",
      "device.isMobile                                    903654 non-null bool\n",
      "device.operatingSystem                             903654 non-null int64\n",
      "device_deviceCategory_channelGrouping              903654 non-null int64\n",
      "fullVisitorId                                      903654 non-null object\n",
      "geoNetwork.city                                    903654 non-null int64\n",
      "geoNetwork.city_device.browser                     903654 non-null int64\n",
      "geoNetwork.city_device.deviceCategory              903654 non-null int64\n",
      "geoNetwork.city_device.operatingSystem             903654 non-null int64\n",
      "geoNetwork.city_trafficSource.source               903654 non-null int64\n",
      "geoNetwork.continent                               903654 non-null int64\n",
      "geoNetwork.continent_device.browser                903654 non-null int64\n",
      "geoNetwork.continent_device.deviceCategory         903654 non-null int64\n",
      "geoNetwork.continent_device.operatingSystem        903654 non-null int64\n",
      "geoNetwork.continent_trafficSource.source          903654 non-null int64\n",
      "geoNetwork.country                                 903654 non-null int64\n",
      "geoNetwork.country_device.browser                  903654 non-null int64\n",
      "geoNetwork.country_device.deviceCategory           903654 non-null int64\n",
      "geoNetwork.country_device.operatingSystem          903654 non-null int64\n",
      "geoNetwork.country_trafficSource.source            903654 non-null int64\n",
      "geoNetwork.metro                                   903654 non-null int64\n",
      "geoNetwork.metro_device.browser                    903654 non-null int64\n",
      "geoNetwork.metro_device.deviceCategory             903654 non-null int64\n",
      "geoNetwork.metro_device.operatingSystem            903654 non-null int64\n",
      "geoNetwork.metro_trafficSource.source              903654 non-null int64\n",
      "geoNetwork.networkDomain                           903654 non-null int64\n",
      "geoNetwork.networkDomain_device.browser            903654 non-null int64\n",
      "geoNetwork.networkDomain_device.deviceCategory     903654 non-null int64\n",
      "geoNetwork.networkDomain_device.operatingSystem    903654 non-null int64\n",
      "geoNetwork.networkDomain_trafficSource.source      903654 non-null int64\n",
      "geoNetwork.region                                  903654 non-null int64\n",
      "geoNetwork.region_device.browser                   903654 non-null int64\n",
      "geoNetwork.region_device.deviceCategory            903654 non-null int64\n",
      "geoNetwork.region_device.operatingSystem           903654 non-null int64\n",
      "geoNetwork.region_trafficSource.source             903654 non-null int64\n",
      "geoNetwork.subContinent                            903654 non-null int64\n",
      "geoNetwork.subContinent_device.browser             903654 non-null int64\n",
      "geoNetwork.subContinent_device.deviceCategory      903654 non-null int64\n",
      "geoNetwork.subContinent_device.operatingSystem     903654 non-null int64\n",
      "geoNetwork.subContinent_trafficSource.source       903654 non-null int64\n",
      "medium.source                                      903654 non-null int64\n",
      "sess_date_dow                                      903654 non-null int64\n",
      "sess_date_hours                                    903654 non-null int64\n",
      "sess_month                                         903654 non-null int64\n",
      "sessionId                                          903654 non-null object\n",
      "source.country                                     903654 non-null int64\n",
      "totals.bounces                                     903654 non-null float64\n",
      "totals.hits                                        903654 non-null float64\n",
      "totals.newVisits                                   903654 non-null float64\n",
      "totals.pageviews                                   903654 non-null float64\n",
      "trafficSource.adContent                            903654 non-null int64\n",
      "trafficSource.adwordsClickInfo.adNetworkType       903654 non-null int64\n",
      "trafficSource.adwordsClickInfo.gclId               903654 non-null int64\n",
      "trafficSource.adwordsClickInfo.isVideoAd           903654 non-null int64\n",
      "trafficSource.adwordsClickInfo.page                903654 non-null int64\n",
      "trafficSource.adwordsClickInfo.slot                903654 non-null int64\n",
      "trafficSource.campaign                             903654 non-null int64\n",
      "trafficSource.isTrueDirect                         903654 non-null int64\n",
      "trafficSource.keyword                              903654 non-null int64\n",
      "trafficSource.medium                               903654 non-null int64\n",
      "trafficSource.referralPath                         903654 non-null int64\n",
      "trafficSource.source                               903654 non-null int64\n",
      "visitId                                            903654 non-null object\n",
      "visitNumber                                        903654 non-null int64\n",
      "prev_session                                       903654 non-null int64\n",
      "next_session                                       903654 non-null int64\n",
      "usermean_totals.hits                               903654 non-null float64\n",
      "usermean_totals.pageviews                          903654 non-null float64\n",
      "usermax_visitNumber                                903654 non-null int64\n",
      "hits/pageviews                                     903654 non-null float64\n",
      "is_high_hits                                       903654 non-null int32\n",
      "predictions                                        903654 non-null float64\n",
      "dtypes: bool(1), datetime64[ns](1), float64(13), int32(1), int64(68), object(3)\n",
      "memory usage: 637.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in log1p\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714167, 365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in log1p\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617242, 365)\n",
      "[0]\tvalidation_0-rmse:2.10179\tvalidation_1-rmse:2.10207\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:1.58118\tvalidation_1-rmse:1.60261\n",
      "[200]\tvalidation_0-rmse:1.47411\tvalidation_1-rmse:1.51346\n",
      "[300]\tvalidation_0-rmse:1.42559\tvalidation_1-rmse:1.48608\n",
      "[400]\tvalidation_0-rmse:1.39494\tvalidation_1-rmse:1.4783\n",
      "[500]\tvalidation_0-rmse:1.37323\tvalidation_1-rmse:1.47484\n",
      "[600]\tvalidation_0-rmse:1.35563\tvalidation_1-rmse:1.47409\n",
      "[700]\tvalidation_0-rmse:1.33717\tvalidation_1-rmse:1.47362\n",
      "Stopping. Best iteration:\n",
      "[675]\tvalidation_0-rmse:1.34217\tvalidation_1-rmse:1.47348\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.42432\tVALID's rmse: 1.47121\n",
      "[200]\tTRAIN's rmse: 1.38322\tVALID's rmse: 1.46635\n",
      "Early stopping, best iteration is:\n",
      "[171]\tTRAIN's rmse: 1.39139\tVALID's rmse: 1.46583\n",
      "[0]\tvalidation_0-rmse:2.09336\tvalidation_1-rmse:2.09543\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:1.62194\tvalidation_1-rmse:1.64733\n",
      "[200]\tvalidation_0-rmse:1.48773\tvalidation_1-rmse:1.53398\n",
      "[300]\tvalidation_0-rmse:1.43404\tvalidation_1-rmse:1.5003\n",
      "[400]\tvalidation_0-rmse:1.40584\tvalidation_1-rmse:1.48873\n",
      "[500]\tvalidation_0-rmse:1.38555\tvalidation_1-rmse:1.48518\n",
      "[600]\tvalidation_0-rmse:1.36603\tvalidation_1-rmse:1.4839\n",
      "[700]\tvalidation_0-rmse:1.34895\tvalidation_1-rmse:1.48327\n",
      "Stopping. Best iteration:\n",
      "[681]\tvalidation_0-rmse:1.35237\tvalidation_1-rmse:1.48294\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.4218\tVALID's rmse: 1.48425\n",
      "[200]\tTRAIN's rmse: 1.38097\tVALID's rmse: 1.48036\n",
      "Early stopping, best iteration is:\n",
      "[145]\tTRAIN's rmse: 1.39928\tVALID's rmse: 1.47992\n",
      "[0]\tvalidation_0-rmse:2.09221\tvalidation_1-rmse:2.09606\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:1.59121\tvalidation_1-rmse:1.61791\n",
      "[200]\tvalidation_0-rmse:1.46806\tvalidation_1-rmse:1.52015\n",
      "[300]\tvalidation_0-rmse:1.42134\tvalidation_1-rmse:1.49352\n",
      "[400]\tvalidation_0-rmse:1.39221\tvalidation_1-rmse:1.4852\n",
      "[500]\tvalidation_0-rmse:1.37067\tvalidation_1-rmse:1.48204\n",
      "[600]\tvalidation_0-rmse:1.35181\tvalidation_1-rmse:1.48098\n",
      "[700]\tvalidation_0-rmse:1.33724\tvalidation_1-rmse:1.48138\n",
      "Stopping. Best iteration:\n",
      "[668]\tvalidation_0-rmse:1.34312\tvalidation_1-rmse:1.48071\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.42357\tVALID's rmse: 1.47706\n",
      "[200]\tTRAIN's rmse: 1.38089\tVALID's rmse: 1.47531\n",
      "Early stopping, best iteration is:\n",
      "[140]\tTRAIN's rmse: 1.40161\tVALID's rmse: 1.47403\n",
      "[0]\tvalidation_0-rmse:2.09561\tvalidation_1-rmse:2.08198\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:1.60862\tvalidation_1-rmse:1.61258\n",
      "[200]\tvalidation_0-rmse:1.47125\tvalidation_1-rmse:1.50259\n",
      "[300]\tvalidation_0-rmse:1.42424\tvalidation_1-rmse:1.47637\n",
      "[400]\tvalidation_0-rmse:1.39608\tvalidation_1-rmse:1.4681\n",
      "[500]\tvalidation_0-rmse:1.37465\tvalidation_1-rmse:1.46595\n",
      "[600]\tvalidation_0-rmse:1.35376\tvalidation_1-rmse:1.46526\n",
      "Stopping. Best iteration:\n",
      "[575]\tvalidation_0-rmse:1.35776\tvalidation_1-rmse:1.46488\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.42715\tVALID's rmse: 1.45537\n",
      "[200]\tTRAIN's rmse: 1.38457\tVALID's rmse: 1.45347\n",
      "Early stopping, best iteration is:\n",
      "[143]\tTRAIN's rmse: 1.40374\tVALID's rmse: 1.45301\n",
      "[0]\tvalidation_0-rmse:2.09121\tvalidation_1-rmse:2.10016\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-rmse:1.58326\tvalidation_1-rmse:1.62025\n",
      "[200]\tvalidation_0-rmse:1.47387\tvalidation_1-rmse:1.53211\n",
      "[300]\tvalidation_0-rmse:1.42696\tvalidation_1-rmse:1.50426\n",
      "[400]\tvalidation_0-rmse:1.39565\tvalidation_1-rmse:1.49516\n",
      "[500]\tvalidation_0-rmse:1.37556\tvalidation_1-rmse:1.49159\n",
      "[600]\tvalidation_0-rmse:1.3533\tvalidation_1-rmse:1.49025\n",
      "[700]\tvalidation_0-rmse:1.33417\tvalidation_1-rmse:1.48873\n",
      "[800]\tvalidation_0-rmse:1.31642\tvalidation_1-rmse:1.48808\n",
      "Stopping. Best iteration:\n",
      "[781]\tvalidation_0-rmse:1.32028\tvalidation_1-rmse:1.48792\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tTRAIN's rmse: 1.42169\tVALID's rmse: 1.48315\n",
      "[200]\tTRAIN's rmse: 1.37992\tVALID's rmse: 1.48087\n",
      "Early stopping, best iteration is:\n",
      "[163]\tTRAIN's rmse: 1.39141\tVALID's rmse: 1.48022\n",
      "1.4706342919878999\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data at User level\n",
    "trn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "\n",
    "# Create a list of predictions for each Visitor\n",
    "trn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "# Create a DataFrame with VisitorId as index\n",
    "# trn_pred_list contains dict \n",
    "# so creating a dataframe from it will expand dict values into columns\n",
    "trn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\n",
    "trn_feats = trn_all_predictions.columns\n",
    "trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\n",
    "trn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\n",
    "trn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\n",
    "trn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "trn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "full_data = pd.concat([trn_data, trn_all_predictions], axis=1)\n",
    "del trn_data, trn_all_predictions\n",
    "gc.collect()\n",
    "print(full_data.shape)\n",
    "\n",
    "\n",
    "sub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n",
    "sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n",
    "for f in trn_feats:\n",
    "    if f not in sub_all_predictions.columns:\n",
    "        sub_all_predictions[f] = np.nan\n",
    "sub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\n",
    "sub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\n",
    "sub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\n",
    "sub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "sub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\n",
    "del sub_data, sub_all_predictions\n",
    "gc.collect()\n",
    "print(sub_full_data.shape)\n",
    "\n",
    "# Create target at Visitor level\n",
    "train['target'] = y_reg\n",
    "trn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()\n",
    "\n",
    "# Train a model at Visitor level\n",
    "folds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n",
    "\n",
    "oof_preds = np.zeros(full_data.shape[0])\n",
    "oof_preds1 = np.zeros(full_data.shape[0])\n",
    "sub_preds = np.zeros(sub_full_data.shape[0])\n",
    "both_oof = np.zeros(full_data.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "        'objective': 'reg:linear',\n",
    "        'booster': 'gbtree',\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 22,\n",
    "        'min_child_weight': 57,\n",
    "        'gamma' : 1.45,\n",
    "        'alpha': 0.0,\n",
    "        'lambda': 0.0,\n",
    "        'subsample': 0.67,\n",
    "        'colsample_bytree': 0.054,\n",
    "        'colsample_bylevel': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 456\n",
    "    }\n",
    "\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    \n",
    "    xg = XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        num_leaves=31,\n",
    "        learning_rate=0.03,\n",
    "        n_estimators=1000,\n",
    "        subsample=.9,\n",
    "        colsample_bytree=.9,\n",
    "        random_state=1\n",
    "    )\n",
    "    \n",
    "    xg.fit(trn_x,np.log1p(trn_y),\n",
    "           eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "           early_stopping_rounds=100,\n",
    "           eval_metric='rmse',\n",
    "           verbose=100     \n",
    "          )\n",
    "    \n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        eval_names=['TRAIN', 'VALID'],\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = trn_x.columns\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_preds1[val_] = xg.predict(val_x)\n",
    "    oof_preds[oof_preds < 0] = 0\n",
    "    oof_preds1[oof_preds1 < 0] = 0\n",
    "    \n",
    "    both_oof[val_] = oof_preds[val_] * 0.6 + oof_preds1[val_] * 0.4\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    \n",
    "    pre = xg.predict(sub_full_data[full_data.columns])\n",
    "    pre[pre<0]=0\n",
    "    \n",
    "    sub_preds += (_preds / len(folds)) * 0.6 + (pre / len(folds)) * 0.4\n",
    "    \n",
    "print(mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5)\n",
    "\n",
    "# Display feature importances\n",
    "# vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\n",
    "# mean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\n",
    "# vis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "# plt.figure(figsize=(8, 25))\n",
    "# sns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])\n",
    "\n",
    "# Save predictions\n",
    "sub_full_data['PredictedLogRevenue'] = sub_preds\n",
    "sub_full_data[['PredictedLogRevenue']].to_csv('submission.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
