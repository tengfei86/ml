{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_df.pickle', 'test.csv', 'extracted_fields_train.gz', 'Train_external_data_2.csv', 'extracted_fields_test.gz', 'Test_external_data.csv', 'train.csv', 'Train_external_data.csv', 'train_df.pickle', 'Test_external_data_2.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas.io.json import json_normalize\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold,GridSearchCV\n",
    "gc.enable()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train.csv. Shape: (903653, 55)\n",
      "Loaded test.csv. Shape: (804684, 53)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    train = pd.read_csv('./all/extracted_fields_train.gz', dtype = {'date' : str, 'fullVisitorId' : str, 'sessionId' : str},nrows = None)\n",
    "    test = pd.read_csv('./all/extracted_fields_test.gz', dtype = {'date': str, 'fullVisitorId': str, 'sessionId': str },nrows = None)\n",
    "    return train,test\n",
    "colab=False\n",
    "if colab:\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "\n",
    "    train_downloaded = drive.CreateFile({'id': \"1JXqmDwX8lKX8cb_GC3qm-PdcXUdE6AgF\"})\n",
    "    train_downloaded.GetContentFile('extracted_fields_train.gz')\n",
    "    test_downloaded = drive.CreateFile({'id': \"1ga8aiZVkLOaRMmlb_uccG6gDt4J6zVtE\"})\n",
    "    test_downloaded.GetContentFile('extracted_fields_test.gz')\n",
    "    train_store_1_downloaded = drive.CreateFile({'id': \"13zEq_SMdADWzqNwLnX1Fk3Q4YjjCB2xe\"})\n",
    "    train_store_1_downloaded.GetContentFile('Train_external_data.csv')\n",
    "    train_store_2_downloaded = drive.CreateFile({'id': \"1eCPXb1aqWj223gMAlv7Ut8D3w3sAkUWh\"})\n",
    "    train_store_2_downloaded.GetContentFile('Train_externa2_data.csv')\n",
    "    test_store_1_downloaded = drive.CreateFile({'id': \"1LNuY7J5BcjyM1Mz--PsPXWKJZrWlne-s\"})\n",
    "    test_store_1_downloaded.GetContentFile('Test_external_data.csv')\n",
    "    test_store_2_downloaded = drive.CreateFile({'id': \"169vfiQJK4WMM09QxAD_dKAq8Gy4swYie\"})\n",
    "    test_store_2_downloaded.GetContentFile('Test_external_data_2.csv')\n",
    "\n",
    "    # https://drive.google.com/open?id=1GXjSajN2Yy5TuxDHfZtvjyFb7lmdezyH\n",
    "    train = pd.read_csv('extracted_fields_train.gz', dtype = {'date' : str, 'fullVisitorId' : str, 'sessionId' : str},nrows = None)\n",
    "    test = pd.read_csv('extracted_fields_test.gz', dtype = {'date': str, 'fullVisitorId': str, 'sessionId': str },nrows = None)\n",
    "    train_store_1 = pd.read_csv('Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    train_store_2 = pd.read_csv('Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    test_store_1 = pd.read_csv('Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    test_store_2 = pd.read_csv('Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "\n",
    "else:\n",
    "    train,test = load_data()\n",
    "    #Loading external data\n",
    "    train_store_1 = pd.read_csv('./all/Train_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    train_store_2 = pd.read_csv('./all/Train_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    test_store_1 = pd.read_csv('./all/Test_external_data.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "    test_store_2 = pd.read_csv('./all/Test_external_data_2.csv', low_memory=False, skiprows=6, dtype={\"Client Id\":'str'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(df=None,n_splits = 5):\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "    folds = GroupKFold(n_splits = n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis,val_vis in folds.split(X=unique_vis,y = unique_vis,groups = unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "    return fold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 target Defind transaction\n",
    "y_reg = train['totals.transactionRevenue'].fillna(0)\n",
    "del train['totals.transactionRevenue']\n",
    "\n",
    "if 'totals.transactionRevenue' in test.columns:\n",
    "    del test['totals.transactionRevenue']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = y_reg\n",
    "for df in [train,test]:\n",
    "    df['date'] = pd.to_datetime(df['visitStartTime'],unit = 's')\n",
    "    df['sess_date_dow'] = df['date'].dt.dayofweek\n",
    "    df['sess_date_hours'] = df['date'].dt.hour\n",
    "    df['sess_date_dom'] = df['date'].dt.day\n",
    "    \n",
    "    # future feature\n",
    "    df.sort_values(['fullVisitorId', 'date'], ascending=True, inplace=True)\n",
    "    df['next_session_1'] = (\n",
    "        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    df['next_session_2'] = (\n",
    "        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    \n",
    "    df['next_session_3'] = (\n",
    "        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(2)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    df['next_session_4'] = (\n",
    "        df['date'] - df[['fullVisitorId', 'date']].groupby('fullVisitorId')['date'].shift(-2)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    \n",
    "    \n",
    "#     df['nb_pageviews'] = df['date'].map(df[['date','totals.pageviews']].groupby('date')['totals.pageviews'].sum())    \n",
    "#     df['ratio_pageviews'] = df['totals.pageviews'] / df['nb_pageviews']\n",
    "    \n",
    "y_reg = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903654,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [train, test]:\n",
    "#     df['vis_date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n",
    "#     df['sess_date_dow'] = df['vis_date'].dt.dayofweek\n",
    "#     df['sess_date_hours'] = df['vis_date'].dt.hour\n",
    "#     df['sess_date_dom'] = df['vis_date'].dt.day\n",
    "#     df.sort_values(['fullVisitorId', 'vis_date'], ascending=True, inplace=True)\n",
    "#     df['next_session_1'] = (\n",
    "#         df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(1)\n",
    "#     ).astype(np.int64) // 1e9 // 60 // 60\n",
    "#     df['next_session_2'] = (\n",
    "#         df['vis_date'] - df[['fullVisitorId', 'vis_date']].groupby('fullVisitorId')['vis_date'].shift(-1)\n",
    "#     ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    \n",
    "# #     df['max_visits'] = df['fullVisitorId'].map(\n",
    "# #         df[['fullVisitorId', 'visitNumber']].groupby('fullVisitorId')['visitNumber'].max()\n",
    "# #     )\n",
    "    \n",
    "#     df['nb_pageviews'] = df['date'].map(\n",
    "#         df[['date', 'totals.pageviews']].groupby('date')['totals.pageviews'].sum()\n",
    "#     )\n",
    "    \n",
    "#     df['ratio_pageviews'] = df['totals.pageviews'] / df['nb_pageviews']\n",
    "    \n",
    "# #     df['nb_sessions'] = df['date'].map(\n",
    "# #         df[['date']].groupby('date').size()\n",
    "# #     )\n",
    "    \n",
    "# #     df['nb_sessions_28_ma'] = df['date'].map(\n",
    "# #         df[['date']].groupby('date').size().rolling(28, min_periods=7).mean()\n",
    "# #     )\n",
    "\n",
    "# #     df['nb_sessions_28_ma'] = df['nb_sessions'] / df['nb_sessions_28_ma']\n",
    "\n",
    "# #     df['nb_sessions_per_day'] = df['date'].map(\n",
    "# #         df[['date']].groupby('date').size()\n",
    "# #     )\n",
    "    \n",
    "# #     df['nb_visitors_per_day'] = df['date'].map(\n",
    "# #         df[['date','fullVisitorId']].groupby('date')['fullVisitorId'].nunique()\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_mapping(x):\n",
    "    browsers = ['chrome','safari','firefox','internet explorer','edge','opera','coc coc','maxthon','iron']\n",
    "    if x in browsers:\n",
    "        return x.lower()\n",
    "    elif  ('android' in x) or ('samsung' in x) or ('mini' in x) or ('iphone' in x) or ('in-app' in x) or ('playstation' in x):\n",
    "        return 'mobile browser'\n",
    "    elif  ('mozilla' in x) or ('chrome' in x) or ('blackberry' in x) or ('nokia' in x) or ('browser' in x) or ('amazon' in x):\n",
    "        return 'mobile browser'\n",
    "    elif  ('lunascape' in x) or ('netscape' in x) or ('blackberry' in x) or ('konqueror' in x) or ('puffin' in x) or ('amazon' in x):\n",
    "        return 'mobile browser'\n",
    "    elif '(not set)' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adcontents_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('placement' in x) | ('placememnt' in x):\n",
    "        return 'placement'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def source_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('youtube' in x):\n",
    "        return 'youtube'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'yahoo' in x:\n",
    "        return 'yahoo'\n",
    "    elif 'facebook' in x:\n",
    "        return 'facebook'\n",
    "    elif 'reddit' in x:\n",
    "        return 'reddit'\n",
    "    elif 'bing' in x:\n",
    "        return 'bing'\n",
    "    elif 'quora' in x:\n",
    "        return 'quora'\n",
    "    elif 'outlook' in x:\n",
    "        return 'outlook'\n",
    "    elif 'linkedin' in x:\n",
    "        return 'linkedin'\n",
    "    elif 'pinterest' in x:\n",
    "        return 'pinterest'\n",
    "    elif 'ask' in x:\n",
    "        return 'ask'\n",
    "    elif 'siliconvalley' in x:\n",
    "        return 'siliconvalley'\n",
    "    elif 'lunametrics' in x:\n",
    "        return 'lunametrics'\n",
    "    elif 'amazon' in x:\n",
    "        return 'amazon'\n",
    "    elif 'mysearch' in x:\n",
    "        return 'mysearch'\n",
    "    elif 'qiita' in x:\n",
    "        return 'qiita'\n",
    "    elif 'messenger' in x:\n",
    "        return 'messenger'\n",
    "    elif 'twitter' in x:\n",
    "        return 'twitter'\n",
    "    elif 't.co' in x:\n",
    "        return 't.co'\n",
    "    elif 'vk.com' in x:\n",
    "        return 'vk.com'\n",
    "    elif 'search' in x:\n",
    "        return 'search'\n",
    "    elif 'edu' in x:\n",
    "        return 'edu'\n",
    "    elif 'mail' in x:\n",
    "        return 'mail'\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    elif 'golang' in x:\n",
    "        return 'golang'\n",
    "    elif 'direct' in x:\n",
    "        return 'direct'\n",
    "    elif 'dealspotr' in x:\n",
    "        return 'dealspotr'\n",
    "    elif 'sashihara' in x:\n",
    "        return 'sashihara'\n",
    "    elif 'phandroid' in x:\n",
    "        return 'phandroid'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'mdn' in x:\n",
    "        return 'mdn'\n",
    "    elif 'duckduckgo' in x:\n",
    "        return 'duckduckgo'\n",
    "    elif 'seroundtable' in x:\n",
    "        return 'seroundtable'\n",
    "    elif 'metrics' in x:\n",
    "        return 'metrics'\n",
    "    elif 'sogou' in x:\n",
    "        return 'sogou'\n",
    "    elif 'businessinsider' in x:\n",
    "        return 'businessinsider'\n",
    "    elif 'github' in x:\n",
    "        return 'github'\n",
    "    elif 'gophergala' in x:\n",
    "        return 'gophergala'\n",
    "    elif 'yandex' in x:\n",
    "        return 'yandex'\n",
    "    elif 'msn' in x:\n",
    "        return 'msn'\n",
    "    elif 'dfa' in x:\n",
    "        return 'dfa'\n",
    "    elif '(not set)' in x:\n",
    "        return '(not set)'\n",
    "    elif 'feedly' in x:\n",
    "        return 'feedly'\n",
    "    elif 'arstechnica' in x:\n",
    "        return 'arstechnica'\n",
    "    elif 'squishable' in x:\n",
    "        return 'squishable'\n",
    "    elif 'flipboard' in x:\n",
    "        return 'flipboard'\n",
    "    elif 't-online.de' in x:\n",
    "        return 't-online.de'\n",
    "    elif 'sm.cn' in x:\n",
    "        return 'sm.cn'\n",
    "    elif 'wow' in x:\n",
    "        return 'wow'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'partners' in x:\n",
    "        return 'partners'\n",
    "    else:\n",
    "        return 'others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing category feature\n",
    "train['device.browser'] = train['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\n",
    "train['trafficSource.adContent'] = train['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "train['trafficSource.source'] = train['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n",
    "\n",
    "test['device.browser'] = test['device.browser'].map(lambda x:browser_mapping(str(x).lower())).astype('str')\n",
    "test['trafficSource.adContent'] = test['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "test['trafficSource.source'] = test['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process device ...\n",
      "process device ...\n"
     ]
    }
   ],
   "source": [
    "# interaction new category feature\n",
    "\n",
    "def process_device(data_df):\n",
    "    print(\"process device ...\")\n",
    "    data_df['source.country'] = data_df['trafficSource.source'] + '_' + data_df['geoNetwork.country']\n",
    "    data_df['campaign.medium'] = data_df['trafficSource.campaign'] + '_' + data_df['trafficSource.medium']\n",
    "    data_df['browser.category'] = data_df['device.browser'] + '_' + data_df['device.deviceCategory']\n",
    "    data_df['browser.os'] = data_df['device.browser'] + '_' + data_df['device.operatingSystem']\n",
    "    return data_df\n",
    "\n",
    "train = process_device(train)\n",
    "test = process_device(test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom..\n",
      "custom..\n"
     ]
    }
   ],
   "source": [
    "def custom(data):\n",
    "    print('custom..')\n",
    "    data['device_deviceCategory_channelGrouping'] = data['device.deviceCategory'] + \"_\" + data['channelGrouping']\n",
    "    data['channelGrouping_browser'] = data['device.browser'] + \"_\" + data['channelGrouping']\n",
    "    data['channelGrouping_OS'] = data['device.operatingSystem'] + \"_\" + data['channelGrouping']\n",
    "    \n",
    "    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n",
    "        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n",
    "            data[i + \"_\" + j] = data[i] + \"_\" + data[j]\n",
    "    \n",
    "    data['content.source'] = data['trafficSource.adContent'] + \"_\" + data['source.country']\n",
    "    data['medium.source'] = data['trafficSource.medium'] + \"_\" + data['source.country']\n",
    "    return data\n",
    "\n",
    "train = custom(train)\n",
    "test = custom(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Encoding\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    \n",
    "#     corr = np.corrcoef(target.values, encoded_feature)[0][1]\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_encoding(df, col):\n",
    "    freq_encoding = df.groupby([col]).size()/df.shape[0] \n",
    "    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n",
    "    return df.merge(freq_encoding, on=col, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature list\n",
    "excluded_features = [\n",
    "    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n",
    "    'visitId', 'visitStartTime','vis_date'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    _f for _f in train.columns\n",
    "    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n",
    "]\n",
    "     \n",
    "# target_encoding = [col for col in categorical_features_1 if train[col].nunique() > 30 ]\n",
    "# print(\"target_encoding\",target_encoding)\n",
    "# del categorical_features_1\n",
    "target_encoding = categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Factorize categoricals\n",
    "\n",
    "for f in categorical_features:\n",
    "    train[f], indexer = pd.factorize(train[f])\n",
    "    test[f] = indexer.get_indexer(test[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_train = train.shape[0]\n",
    "# df_all = pd.concat([train,test])\n",
    "\n",
    "# for col in tqdm(['medium.source']):\n",
    "#     df_all = frequency_encoding(df_all,col)\n",
    "# train = df_all[:len_train]\n",
    "# test  = df_all[len_train:]\n",
    "\n",
    "# train = df_train.copy()\n",
    "# test = df_test.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in target_encoding:\n",
    "#     train_enc ,test_enc = target_encode(train[col],test[col],y_reg)\n",
    "#     train = pd.concat([train,train_enc],axis = 1)\n",
    "#     test = pd.concat([test,test_enc],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((903654, 86), (804686, 86))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'learning_rate': 0.03,\n",
    "        'objective':'regression',\n",
    "        'metric':'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'verbose': 1,\n",
    "        \"subsample\": 0.90,\n",
    "        \"colsample_bytree\": 0.90,\n",
    "        \"random_state\":42,\n",
    "        'max_depth': 15,\n",
    "        'lambda_l2': 0.02585548700474218,\n",
    "        'lambda_l1': 0.02107624022751344,\n",
    "        'bagging_fraction': 0.7934712636944741,\n",
    "        'feature_fraction': 0.686612409641711,\n",
    "        'min_child_samples': 21\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'log1p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b03c3ecadc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     reg.fit(\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'log1p'"
     ]
    }
   ],
   "source": [
    "folds = get_folds(df=train, n_splits=5)\n",
    "\n",
    "train_features = [_f for _f in train.columns if _f not in excluded_features]\n",
    "# print(train_features)\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "oof_reg_preds = np.zeros(train.shape[0])\n",
    "sub_reg_preds = np.zeros(test.shape[0])\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    print(\"Fold:\",fold_)\n",
    "    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n",
    "    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n",
    "    reg = lgb.LGBMRegressor(**params,\n",
    "         n_estimators=1000\n",
    "    )\n",
    "    \n",
    "#     reg = lgb.LGBMRegressor(\n",
    "#         num_leaves=31,\n",
    "#         learning_rate=0.03,\n",
    "#         n_estimators=1000,\n",
    "#         subsample=.9,\n",
    "#         colsample_bytree=.9,\n",
    "#         random_state=1\n",
    "#     )\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = train_features\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_reg_preds[oof_reg_preds < 0] = 0\n",
    "    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    sub_reg_preds += np.expm1(_preds) / len(folds)\n",
    "    \n",
    "mean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.Display feature importances\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "importances['gain_log'] = np.log1p(importances['gain'])\n",
    "mean_gain = importances[['gain', 'feature']].groupby('feature').mean()\n",
    "importances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predictions'] = np.expm1(oof_reg_preds)\n",
    "test['predictions'] = sub_reg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data at User level\n",
    "trn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create a list of predictions for each Visitor\n",
    "trn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.predictions))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_all_predictions = pd.DataFrame(list(trn_pred_list.values),index = trn_data.index)\n",
    "trn_feats = trn_all_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\n",
    "trn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\n",
    "trn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\n",
    "trn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_all_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([trn_data,trn_all_predictions],axis = 1)\n",
    "del trn_data, trn_all_predictions\n",
    "gc.collect()\n",
    "full_data.shape\n",
    "full_data.to_csv('full_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub_pred_list = test[['fullVisitorId','predictions']].groupby('fullVisitorId').apply(lambda df: list(df.predictions)).apply(lambda x: {'pred_' + str(i): pred for i,pred in enumerate(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\n",
    "sub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\n",
    "for f in trn_feats:\n",
    "    if f not in sub_all_predictions.columns:\n",
    "        sub_all_predictions[f] = np.nan\n",
    "sub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\n",
    "sub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\n",
    "sub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\n",
    "sub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\n",
    "sub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\n",
    "#sub_all_predictions.to_csv('sub_all_predictions.csv',index = False)\n",
    "sub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\n",
    "del sub_data, sub_all_predictions\n",
    "gc.collect()\n",
    "sub_full_data.shape\n",
    "sub_full_data.to_csv('sub_full_data.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 Create target and Cross Validation\n",
    "train['target'] = y_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_target = train[['fullVisitorId','target']].groupby('fullVisitorId').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 Train a model at Visitor level\n",
    "params={'learning_rate': 0.03,\n",
    "        'objective':'regression',\n",
    "        'metric':'rmse',\n",
    "        'num_leaves': 31,\n",
    "        'verbose': 1,\n",
    "        \"subsample\": 0.99,\n",
    "        \"colsample_bytree\": 0.99,\n",
    "        \"random_state\":42,\n",
    "        'max_depth': 15,\n",
    "        'lambda_l2': 0.02085548700474218,\n",
    "        'lambda_l1': 0.004107624022751344,\n",
    "        'bagging_fraction': 0.7934712636944741,\n",
    "        'feature_fraction': 0.686612409641711,\n",
    "        'min_child_samples': 21\n",
    "       }\n",
    "\n",
    "xgb_params = {\n",
    "        'objective': 'reg:linear',\n",
    "        'booster': 'gbtree',\n",
    "        'learning_rate': 0.02,\n",
    "        'max_depth': 22,\n",
    "        'min_child_weight': 57,\n",
    "        'gamma' : 1.45,\n",
    "        'alpha': 0.0,\n",
    "        'lambda': 0.0,\n",
    "        'subsample': 0.67,\n",
    "        'colsample_bytree': 0.054,\n",
    "        'colsample_bylevel': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 456\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "folds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n",
    "\n",
    "oof_preds = np.zeros(full_data.shape[0])\n",
    "oof_preds1 = np.zeros(full_data.shape[0])\n",
    "both_oof = np.zeros(full_data.shape[0])\n",
    "sub_preds = np.zeros(sub_full_data.shape[0])\n",
    "vis_importances = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_, val_) in enumerate(folds):\n",
    "    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n",
    "    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n",
    "    \n",
    "    xg = XGBRegressor(**xgb_params, n_estimators=1000)\n",
    "    \n",
    "    reg = lgb.LGBMRegressor(**params,\n",
    "        n_estimators=1500,\n",
    "    )\n",
    "    print('XGB' + \"-\" * 50)\n",
    "    xg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    print('LGB' + \"-\" * 50)\n",
    "    reg.fit(\n",
    "        trn_x, np.log1p(trn_y),\n",
    "        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "        eval_names=['TRAIN', 'VALID'],\n",
    "        early_stopping_rounds=50,\n",
    "        eval_metric='rmse',\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = trn_x.columns\n",
    "    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "    \n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "    \n",
    "    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "    oof_preds1[val_] = xg.predict(val_x)\n",
    "    \n",
    "    oof_preds[oof_preds < 0] = 0\n",
    "    oof_preds1[oof_preds1 < 0] = 0\n",
    "    \n",
    "    both_oof[val_] = oof_preds[val_] * 0.6 + oof_preds1[val_] * 0.4\n",
    "    \n",
    "    # Make sure features are in the same order\n",
    "    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n",
    "    _preds[_preds < 0] = 0\n",
    "    \n",
    "    pre = xg.predict(sub_full_data[full_data.columns])\n",
    "    pre[pre<0]=0\n",
    "    \n",
    "    sub_preds += (_preds / len(folds)) * 0.6 + (pre / len(folds)) * 0.4\n",
    "    \n",
    "print(\"LGB  \", mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5)\n",
    "print(\"XGB  \", mean_squared_error(np.log1p(trn_user_target['target']), oof_preds1) ** .5)\n",
    "print(\"Combine  \", mean_squared_error(np.log1p(trn_user_target['target']), both_oof) ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_full_data['PredictedLogRevenue'] = sub_preds\n",
    "sub_full_data[['PredictedLogRevenue']].to_csv('LGB_XGB.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
